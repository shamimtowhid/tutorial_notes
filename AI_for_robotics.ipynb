{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all my notes from Udacity Course [Artificial Intelligence for Robotics](https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373).  I have learned a lots of fundamental concept of autonomous driving from this course and I am thankful to Udacity for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Localization is the process, how a robot identify its location in an environment. A robot does this using probability theory. Initially the robot does not know where it is, so the probability of being in any place will be the same. In localization, basically we try to change these probabilities with the measurement of the robot such as the probability of one place increases whereas probability of other places decreases. The location with highest probability is the location of the robot and thus our robot will localize itself in the environment. \n",
    "\n",
    "##### steps in localization are:\n",
    "\n",
    "   1. Initialize all the locations with uniform probability distribution, which is called prior belief.\n",
    "   2. sense the world and change our prior belief according to the sense, which is called posterior belief. To change our prior we have to multiply it with the factor of the sense being correct or wrong. Further we have to normalize this to make it a valid probability distribution.   \n",
    "   3. Take an action and change the posterior belief according to the action taken, which is called convolution. \n",
    "   4. sense the world again and now changing our prior belief from previous step will do the trick, now our posterior belief will be something meaningfull. Only one location will have the highest probability. So thus our robot will localize itself. \n",
    "\n",
    "To understand it, lets assume a robot in one dimensional discrete world. We can define the location of this world with different colors and our robot can sense these colors. Now we will try to apply the above algorithm on this robot and lets whether our robot can localize itself or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13636363636363638, 0.13636363636363638, 0.4090909090909091, 0.13636363636363638, 0.04545454545454547, 0.13636363636363638]\n"
     ]
    }
   ],
   "source": [
    "# one dimensional world\n",
    "world = ['red', 'green', 'green','red', 'red', 'green']\n",
    "# step 1: Initializing all the locations with uniform probability\n",
    "p = [1/len(world) for _ in range(len(world))]\n",
    "# These are the factor of being the sense correct or not \n",
    "pHit = 0.6 \n",
    "pMiss = 0.2\n",
    "\n",
    "# These are the valid action the robot can take\n",
    "action = [0,1] #0 means move left and 1 means move right\n",
    "\n",
    "# we will do step 2 as a function because we have to do it repeatedly.\n",
    "def sense(sense, prior):\n",
    "    '''This function can calculate posterior probability given the sense and prior'''\n",
    "    posterior = [0.0 for _ in range(len(prior))]\n",
    "#     chaning prior according to the factor\n",
    "    for i in range(len(prior)):\n",
    "        if sense==world[i]:\n",
    "            posterior[i] = prior[i] * pHit\n",
    "        else:\n",
    "            posterior[i] = prior[i] * pMiss\n",
    "#     normalizing the posterior to make it a valid probability distribution\n",
    "    posterior = [p/sum(posterior) for p in posterior]\n",
    "    return posterior\n",
    "\n",
    "\n",
    "# we will do step 3 as a function too, cause we have to do it repeatedly too. \n",
    "def convolution(action, posterior):\n",
    "    '''This function will change the posterior according to the action taken by the robot.\n",
    "    This function assumes the world to be a cyclic world that means the element falls off \n",
    "    from right will go to the left or vice versa'''\n",
    "    new_prior = [0.0 for _ in range(len(posterior))]\n",
    "#   robot moves to the left\n",
    "    if action == 0:\n",
    "        new_prior = [posterior[(i+1)%len(posterior)] for i in range(len(posterior))]\n",
    "#     robot moves to the right\n",
    "    else:\n",
    "        new_prior = [posterior[(i-1)] for i in range(len(posterior))]\n",
    "    return new_prior\n",
    "\n",
    "# applying the algorithm\n",
    "# step 2\n",
    "posterior = sense('green',p)\n",
    "# step 3\n",
    "new_prior = convolution(1, posterior)\n",
    "# step 2 repeated\n",
    "new_posterior = sense('green',new_prior)\n",
    "print(new_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the highest probability is in location 2(starting from 0 location) which is correct according to our world setup, if we sense two times and green is the result for each time. So by applying the above algorithm our robot can actually localize itself in this one dimensional world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inexact robot motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have undersatnd the basic concepts of localization. But in our previous example we assume the robot to be in exact motion. Which means if the robot wants to go to right by one cell it does it correctly every time. This is not the case in real life. In reality, the robot motion is uncertain. \n",
    "\n",
    "So for example, lets say if we give the robot the command to move right by one cell, the robot remains in the current cell with probability of 0.1, it moves to the right cell with probability of 0.8, it can also overshoot the goal by one cell with probability of 0.1. \n",
    "\n",
    "Considering this uncertain robot motion, the new probability of a cell will be the addition of all the probabilities from cells those are candidate for the current cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21157894736842103, 0.1515789473684211, 0.08105263157894739, 0.16842105263157897, 0.3873684210526316]\n"
     ]
    }
   ],
   "source": [
    "world=['green', 'red', 'red', 'green', 'green']\n",
    "p = [1/len(world) for _ in range(len(world))]\n",
    "measurements = ['red', 'green'] # considering multiple measrements\n",
    "motions = [1, 1] # considering multiple motion\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "# probability for inexact robot motion\n",
    "pExact = 0.8 # probability for the correct move\n",
    "pOvershoot = 0.1 # probability for overshooting the goal \n",
    "pUndershoot = 0.1 # probability for undershooting the goal\n",
    "\n",
    "def sense(sense, prior):\n",
    "    '''This function can calculate posterior probability given the sense and prior'''\n",
    "    posterior = [0.0 for _ in range(len(prior))]\n",
    "#     chaning prior according to the factor\n",
    "    for i in range(len(prior)):\n",
    "        if sense==world[i]:\n",
    "            posterior[i] = prior[i] * pHit\n",
    "        else:\n",
    "            posterior[i] = prior[i] * pMiss\n",
    "#     normalizing the posterior to make it a valid probability distribution\n",
    "    posterior = [p/sum(posterior) for p in posterior]\n",
    "    return posterior\n",
    "\n",
    "def convolution(p, U):\n",
    "    '''This function takes the posterior probability and steps to move in left or right.\n",
    "    It returns new prior distribution. U=1 means move right by one cell, U=-1 means move left \n",
    "    by one cell. Assuming the world to be cyclic'''\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        s = pExact * p[(i-U) % len(p)] # calculating probability of correct motion\n",
    "        s = s + pOvershoot * p[(i-U-1) % len(p)] # probability of overshoot motion\n",
    "        s = s + pUndershoot * p[(i-U+1) % len(p)] # probability of undershoot motion\n",
    "        q.append(s)\n",
    "    return q\n",
    "\n",
    "for i,s in enumerate(measurements):\n",
    "    # step 2\n",
    "    p = sense(s,p)\n",
    "    # step 3\n",
    "    p = convolution(p, motions[i])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above output we can see that the probability distribution works in same way as previous but this time we consider the inexact robot motion. So this convolution function or move function is more accurate considering the real life scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### understand sense and move from probability theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely to the sense function it will lead us to the Bayesian rule. Lets represent the prior belief as X and the measurement as Z then the sense function is calculating the probability after having the measurement Z. In mathmatically we can say it like p(X|Z). We all know that according to the bayes theorem, \n",
    "\n",
    "    p(X|Z) = (p(Z|X) * P(X)) / p(Z)\n",
    "    \n",
    "Here p(Z|X) is the probability of having a measurement. In our case it is defined by pHit and pMiss. p(X) is the prior belief. p(Z) is just a normalized term. Since our final output is a posterior distribution we can replace p(Z) with just the normalization term. Thats the beauty of Bayes rule. So, \n",
    "\n",
    "    p(Z) = sum of p(Z|X) * P(X) for all the cells. \n",
    "    \n",
    "Now the move or convolution function can be relate to something called total probability theory. The way we computed one cell probability after one move, was looking at all the grid cells from which it could have come from one time step earlier, we looked at the prior probability of those grid cells at previous time step and we multiply it with a probability that our motion command would carry us from those cells to this current cell. In probability term people write this like the follows-\n",
    "\n",
    "    p(A) = sum over all B cells ( p(A|B) p(B) ) , here p(B) is the prior at previous time step , p(A|B) is probability of transition to this cell from B cells. \n",
    "    \n",
    "Here A is the current cell index and B is the all possible previous cell's prior probabilities. This theorem is known as Theorem of total probability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above procedure is called histogram based localization or monte carlo robot localization. Next we are going to learn about Kalman filters which is used for tracking other cars in the road. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by using Localization techniques discussed above our robot can find itself in an environment. But for a safe driving we must know the location and velocity of other cars as well in the environment. Kalman filters help use to know the location of other cars in the environment. \n",
    "\n",
    "Though generally Kalman filters are being used to track other cars in the system, it is a similar approach as monte carlo localization. The differences are in Kalman filters, we try to estimate a continuous state (in simple word, we consider the world to be continuous instead of discrete grid cells) whereas in monte carlo localization we estimated a discrete state. As a result Kalman filters gives us unimodal distribution but monte carlo localization gives us multimodal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we consider the state of the robot is now continuous we can represent the prior belief as a gaussian distribution. A gaussian distribution is parameterized by two variable. One is mean and the other is its variance. Since it is representing a probability distribution the area under the gaussian should be sums up to 1. The mean will represent the highest probability for the location. Here the larger the variance is for a distribution the less it is confidence about the probability. So after every measurements we will expect the variance will be very low and we will get a narrow gaussian distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic principle is similar to the monte carlo localization. We will proceed through a measurement and move. Only the mathematical formula will be different now. In the sense function we will update the mean and variance of our belief according to the following formula: \n",
    "\n",
    "    new_mean = (mean * var' + mean' * var) / (var + var'), here mean and var are current belief's parameter and mean' and var' are the measurement distribution's parameter. \n",
    "    \n",
    "    new_var = 1. / (1/var + 1/var')\n",
    "    \n",
    "Here interesting thing to notice is the mean is multiplied with the measurement variance. So the new mean will be much fluenced by the measurement variance. \n",
    "\n",
    "Now the move function updates will be very easy. To get the location after a move we have to just add the moving units with the previous mean. In this way we will get the new location. We can introduce the motion uncertainty using the variance paramter of the action. The new variance will be the sum of the two variance. \n",
    "\n",
    "    new_mean = mean + mean'\n",
    "    new_var = var + var'\n",
    "    \n",
    "In the move function we will update the mean and variance according to the above formulas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10.999906177177365, 4.005861580844194]\n"
     ]
    }
   ],
   "source": [
    "def sense(mean1, var1, mean2, var2):\n",
    "    new_mean = float(var2 * mean1 + var1 * mean2) / (var1 + var2)\n",
    "    new_var = 1./(1./var1 + 1./var2)\n",
    "    return [new_mean, new_var]\n",
    "\n",
    "def move(mean1, var1, mean2, var2):\n",
    "    new_mean = mean1 + mean2\n",
    "    new_var = var1 + var2\n",
    "    return [new_mean, new_var]\n",
    "\n",
    "measurements = [5., 6., 7., 9., 10.]\n",
    "motion = [1., 1., 2., 1., 1.]\n",
    "measurement_sig = 4. # measurement uncertainty\n",
    "motion_sig = 2. # motion uncertainty\n",
    "mu = 0. # initial belief\n",
    "sig = 10000. # initial belief uncertainty\n",
    "\n",
    "for i in range(len(measurements)):\n",
    "    mu,sig = sense(measurements[i],measurement_sig,mu,sig)\n",
    "    mu,sig = move(mu,sig,motion[i],motion_sig)\n",
    "print([mu, sig])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know how to make the measurement and move updates in a continuous environment which represent the state of the robot as gaussian distribution. We consider 1 dimensional environment in the above example but in reality we have multidimensional environment. and things becomes more involved in multidimensional environment. To understand how things work in a multidimensional environment we have to understand multidimensional gaussian which oftens called multivariate gaussian. \n",
    "\n",
    "In a multivariate gaussian the mean is now a vector and variance is represented by a DxD matrix (here D is the dimension) which is called covariance matrix. A two dimensioanl gaussian can be plotted in a contour graph like the follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/contour_kalman_filter.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the center of the contour is representing the mean and the area of the contour represents the covariance matrix. So the more certain we are about one property of the robot, the contour area in that dimension will be very narrow. If the gaussian is tilted a little bit diagonally then the properties represented by that gaussian is correlated. This means if we know about one property we can determine about the other property from the contour graph. This is the true beauty of Kalman filter. If we represent the location in one dimension and the velocity in other dimension. By having the measurement about the location we can infer about the velocity of the robot using Kalman filter.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually all the filters have this property but Kalman filter is very efficient to do this. So whenever we need this kind of thing we use Kalman filter. When we design a Kalman filter we need two things, \n",
    "\n",
    "    1. for the state we need a state transition function, F\n",
    "    2. for the measurement we need a measurement function, H\n",
    "    \n",
    "The sense and move function formula for high dimensional Kalman filter is given below-\n",
    "\n",
    "For sense function: \n",
    "  \n",
    "    x' = F * x + u\n",
    "    p' = F * p * F(transpose)\n",
    "    \n",
    "For move function:\n",
    "\n",
    "    y = z - H * x\n",
    "    S = H * p * H(transpose) + R\n",
    "    K = p * H(transpose) * S(inverse)\n",
    "    x' = x + (K*y)\n",
    "    p' = (I-K*H)*p\n",
    "    \n",
    "Here, \n",
    "\n",
    "    x = estimate\n",
    "    p = uncertainty covariance\n",
    "    F = state transition matrix\n",
    "    u = motion vector\n",
    "    z = measurement\n",
    "    H = measurement function\n",
    "    R = measurement noise\n",
    "    I = identity matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observe  0\n",
      "x =  [[0.999001]\n",
      " [0.      ]]\n",
      "p =  [[1000.999001 1000.      ]\n",
      " [1000.       1000.      ]]\n",
      "Observe  1\n",
      "x =  [[2.99800299]\n",
      " [0.999002  ]]\n",
      "p =  [[4.99002494 2.99301795]\n",
      " [2.99301795 1.99501297]]\n",
      "Observe  2\n",
      "x =  [[3.99966644]\n",
      " [0.99999983]]\n",
      "p =  [[2.33189042 0.99916761]\n",
      " [0.99916761 0.49950058]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def kalman_filter(x, P):\n",
    "    for n in range(len(measurements)):\n",
    "        # measurement update\n",
    "        y = np.array([[measurements[n]]]).transpose() - H * x\n",
    "        S = H * P * H.transpose() + R\n",
    "        K = P * H.transpose() * S.I\n",
    "        x = x + (K*y)\n",
    "        P = (I - K*H) * P\n",
    "        # prediction\n",
    "        x = F * x + u\n",
    "        P = F * P * F.transpose()\n",
    "        print('Observe ',n)\n",
    "        print('x = ', x)\n",
    "        print('p = ', P)\n",
    "    return x,P\n",
    "\n",
    "measurements = [1, 2, 3] # this example consider 1 dimensional motion vector.\n",
    "\n",
    "x = np.matrix([[0.], [0.]]) # initial state (location and velocity)\n",
    "P = np.matrix([[1000., 0.], [0., 1000.]]) # initial uncertainty and there is no coorelation (diagonals are 0.0)\n",
    "u = np.matrix([[0.], [0.]]) # external motion\n",
    "F = np.matrix([[1., 1.], [0, 1.]]) # next state function\n",
    "H = np.matrix([[1., 0.]]) # measurement function\n",
    "R = np.matrix([[1.]]) # measurement uncertainty\n",
    "I = np.matrix([[1., 0.], [0., 1.]]) # identity matrix\n",
    "\n",
    "x, P = kalman_filter(x, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above outputs, we can see that after first measurement update, we observed location 1 which get copied over in the x matrix 0.999001 and nothing about the velocity so it is still 0, the initialized value. and the uncertainty matrix now shows a strong coorelation (1000 in the diagonal element).\n",
    "\n",
    "Then again when we observe location 2, now we can see our next location will be 3 (2.99800 in the matrix). Now we have a really good estimate about the velocity which is 1.0. The reason is the kalman filters were able to use the formula correctly and estimate the velocity. There is also new covariance matrix.\n",
    "\n",
    "And the third observation for location 3, we also get the next prediction correctly for both the location and velocity. We also notice that the covariance matrix has now highest amount of certainty. So the more observation we do the more certain our Kalman filters become about the prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that is how a Kalman filter works and it is very useful in tracking a robot/car. We can also infer about the hidden property of a robot that depends on the observable like the velocity that depends on location. I find it really cool to understand. To understand the formula for prediction and measurement update we have to understand the intution from contour graph plot. \n",
    "\n",
    "So thus we conclude Kalman filters. In terms of efficincy Kalman filter is way better than monte carlo localization because the memory uses increases exponencially with the dimension of the state space. So monte carlo localization is not very good at representing higher dimensional state space. Whereas in Kalman filters the memory uses increases quadratically with dimension. So it is much more efficient than monte carlo localization in terms of scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Particle Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Particle filter is another way for localization. It is very easy to understand and most importantly it is very easy to code. In particle filter we randomly take thousands of guesses about the location of the robot. These are called particles(no surprises there!!).  Then we try to move each particle a little bit and measure the distance from predefined landmarks. Obviously there will be some noise in this measurements and also in the move action. \n",
    "\n",
    "After we get the measurement vector from sensor reading then we try to apply that measurement vector to each particle. Obviously some particles which are in completely different location give us very large error for that measurement vector. Then according to that error we will assign weight to each particle. Thus the particles who are close to the actual location will get high weights and those who are far from actual location will get low weight.\n",
    "\n",
    "Then we do resampling and in this step the particle with larger weight will survive and the particles with lower weight tends to get eliminated. Thus this process repeatedly happend and after few iteration we will be left with very few particles which are very close to the actual location of the robot. This is the basic principle of particle filters. Now we will look into the theory of this resampling process and weight initialization process.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Importance Weight Initialization: </b>\n",
    "\n",
    "The weight for each particle is given based on its distance measure from landmarks. We should also consider the noise in this calculation. The following function will take the actual measurement vector (distance from each landmark) and calculate that particular particle's distance from landmark. Then based on the error it will return weight for that particular particle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.367031293202871e-20\n"
     ]
    }
   ],
   "source": [
    "from math import *\n",
    "\n",
    "landmarks  = [[20.0, 20.0], [80.0, 80.0], [20.0, 80.0], [80.0, 20.0]]\n",
    "world_size = 100.0\n",
    "\n",
    "def cal_distance(src, dest):\n",
    "    return sqrt((src[0] - dest[0]) ** 2 + (src[1] - dest[1]) ** 2)\n",
    "\n",
    "def Gaussian(mu, sigma, x):\n",
    "    # calculates the probability of x for 1-dim Gaussian with mean mu and var. sigma\n",
    "    return exp(- ((mu - x) ** 2) / (sigma ** 2) / 2.0) / sqrt(2.0 * pi * (sigma ** 2))\n",
    "\n",
    "def measurement_prob(particle_loc, measurement):\n",
    "    # measurement is the actual distance\n",
    "    prob = 1.0\n",
    "    for i in range(len(landmarks)):\n",
    "        dist = sqrt((particle_loc[0] - landmarks[i][0]) ** 2 + (particle_loc[1] - landmarks[i][1]) ** 2)\n",
    "        # the more further the actual distance is from the mean of this gaussian\n",
    "        # the less the probability. \n",
    "        prob *= Gaussian(dist, sense_noise, measurement[i])\n",
    "    return prob\n",
    "\n",
    "sense_noise = 5.0 # include the noise for sense \n",
    "robot_loc = [20.0, 30.0] # actual robot location from sense function after the move\n",
    "\n",
    "measurement_vector = [cal_distance(robot_loc, loc) for loc in landmarks]\n",
    "# create one particle with random location after the same amount of move\n",
    "particle_loc = [50.0,30.0]\n",
    "# assign weight to this particle\n",
    "weight = measurement_prob(particle_loc, measurement_vector)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above code snippet we can see that the weight assigned to this particle is very low because clearly it is very far from the actual robot location. Thus every particles will be assigned to some kind of weight based on its location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Resampling: </b>\n",
    "\n",
    "Resampling means we are given N particles, each of which has three values(x coordinate, y coordinate and heading angle) and they also now have weights from previous step. In the first step of resampling we will first normalize the weight values so that we can represent the probability of being selected in the next sample set. Then according to this probability we do randomly sampling with replacement N times. Then the more occuring particles remain in the next sample and the less occuring particle get eliminated. We want to select particles from the set proportional to its probability. How can we do that? \n",
    "\n",
    "The following algorithm does exactly what we want: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'C', 'C', 'A', 'C']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "p3 = []\n",
    "N = 5 # number of particles\n",
    "particles = ['A','B','C','D','E'] # pseudo particles list.\n",
    "# normalized weight or probability of being selected for each particle. Obtained from previous step\n",
    "normalized_weight = [0.1, 0.2, 0.5, 0.1, 0.1] \n",
    "# randomly take an index to start the sampling\n",
    "index = random.randint(0,N-1)\n",
    "beta = 0 # initializing beta with 0\n",
    "for i in range(N):\n",
    "    beta = beta + random.uniform(0,2*max(normalized_weight))\n",
    "    while normalized_weight[index] < beta:\n",
    "        beta = beta - normalized_weight[index]\n",
    "        index = (index + 1)%N\n",
    "    p3.append(particles[index])\n",
    "print(p3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that C is selected 3 times because the probability for C is higher than others. So our algorithm is working perfectly. Now if you look closely then you will see none of the above function or code actually consider the heading direction of the robot. So will they matter eventually? Yes off course they will matter. They will matter in the second step. When we move all the particles, the particles will move based on its direction so in the second step the measurement vector will be different for the particles depending on its heading direction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code shows us particles filter in action. It uses a robot class which is been given in the course. Then it runs the particle filter 10 times to get the approximation of the actual robot position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual robot location: \n",
      "[x=42.475 y=83.451 orient=1.2960]\n",
      "\n",
      "particles approximation\n",
      "[x=41.553 y=85.075 orient=2.2399]\n",
      "[x=41.553 y=85.075 orient=2.2399]\n",
      "[x=41.553 y=85.075 orient=2.2399]\n",
      "[x=41.553 y=85.075 orient=2.2399]\n",
      "[x=41.553 y=85.075 orient=2.2399]\n",
      "[x=41.413 y=80.562 orient=2.4279]\n",
      "[x=41.413 y=80.562 orient=2.4279]\n",
      "[x=41.413 y=80.562 orient=2.4279]\n",
      "[x=41.413 y=80.562 orient=2.4279]\n",
      "[x=41.413 y=80.562 orient=2.4279]\n"
     ]
    }
   ],
   "source": [
    "from utils.robot import *\n",
    "\n",
    "myrobot = robot()\n",
    "myrobot = myrobot.move(0.1, 5.0)\n",
    "Z = myrobot.sense()\n",
    "print('Actual robot location: ')\n",
    "print(myrobot)\n",
    "print()\n",
    "N = 1000\n",
    "p = []\n",
    "for i in range(N):\n",
    "    x = robot()\n",
    "    x.set_noise(0.05, 0.05, 5.0)\n",
    "    p.append(x)\n",
    "    \n",
    "p2 = []\n",
    "for i in range(N):\n",
    "    p2.append(p[i].move(0.1, 5.0))\n",
    "p = p2\n",
    "    \n",
    "for j in range(10):\n",
    "    w = []\n",
    "    for i in range(N):\n",
    "        w.append(p[i].measurement_prob(Z))\n",
    "    \n",
    "    p3 = []\n",
    "    index = int(random.random() * N)\n",
    "    beta = 0.0\n",
    "    mw = max(w)\n",
    "    for i in range(N):\n",
    "        beta += random.random() * 2.0 * mw\n",
    "        while beta > w[index]:\n",
    "            beta -= w[index]\n",
    "            index = (index + 1) % N\n",
    "        p3.append(p[index])\n",
    "    p = p3\n",
    "print('particles approximation')\n",
    "for i in range(10):\n",
    "    print(p[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that after running 10 times the x,y coordinate and the orientation angle for each particle becomes similar and it is also close to the actual robot position. So our particle filter can actually approximate the location of the robot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem we try to solve in motion planning is devise a path for the robot. There is a goal for the robot and the robot start with a initial position. We have to come up with a optimal path from the initial location to the goal state for the robot. There might be a lots of path between a start location and a goal location. The robot has to choose the optimal path based on the situation. This is called planning. \n",
    "\n",
    "So in the motion planning, we are given a map of the environment, the initial location of the robot, the destination and the cost function. Cost function is something that we use for evaluating a solution. Usually we try to find a path which gives us the lowest cost. \n",
    "\n",
    "By manupulating the cost function we can involve real scenario in our motion planning. For example, left turn for an autonomous car is very hard in a real traffic situation. So if we give a large cost for left turn then our algorithm will try to avoid left turn for choosing optimal path. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are so many search algorithms for getting an optimal path. The most common algorithms are greedy algorithms. For example BFS(Breadth First Search) is very popular and simple one. In this algorithm we maintain a list of nodes and expand the lowest cost node first. Nodes are different location that we can actually vist in the graph. Here we are considering the environment to be discrete and representing the entire environment in a tree. \n",
    "\n",
    "In BFS we expand the lowest cost node first until then we check if it is the goal state or not. If it is the goal state then we stop otherwise we continue to exapand lowest cost node. If you notice closely here, during expanding node we do not have any information about the goal position from that particular node so we are expanding based on the cost of coming to that node. Thats why this kind of algorithm sometimes expand many nodes to reach to the goal state. \n",
    "\n",
    "A very popular algorithm called A* is very good at this type of scenario. In A* search we get some information about how far the goal is from expaning node. So we expand those nodes from which the goal state is near. Thus in A* we expand only those nodes which are useful. A* includes this goal information by designing a function called heuristic function. This heuristic function gives us a guess of the distance of goal state from every intermediate state. During expanding a node we add this heuristic value to the cost of that node and expand the node which total cost is the lowest. In this way we avoid the nodes which can give us large cost. \n",
    "\n",
    "If the heuristic value for every node is always lower or equal to the actual cost then that heuristic is called admissible heuristic and it is desired that the heuristic should be admissible for A* to work properly and give us the optimal path always. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is an implementation of A* algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -1, -1, -1, -1, -1]\n",
      "[1, -1, -1, -1, -1, -1]\n",
      "[2, -1, -1, -1, -1, -1]\n",
      "[3, -1, -1, -1, -1, -1]\n",
      "[4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "# the following is the 2D world, 0 means valid position\n",
    "# 1 means obstacles, so we can not go/expand that position \n",
    "grid = [[0, 1, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 0, 0]]\n",
    "# this heuristic is simply the cost to reach the goal from that position \n",
    "heuristic = [[9, 8, 7, 6, 5, 4],\n",
    "             [8, 7, 6, 5, 4, 3],\n",
    "             [7, 6, 5, 4, 3, 2],\n",
    "             [6, 5, 4, 3, 2, 1],\n",
    "             [5, 4, 3, 2, 1, 0]]\n",
    "\n",
    "init = [0, 0] # initial location\n",
    "goal = [len(grid)-1, len(grid[0])-1] # goal location\n",
    "cost = 1 # every action cost 1\n",
    "# actions\n",
    "delta = [[-1, 0 ], # go up\n",
    "         [ 0, -1], # go left\n",
    "         [ 1, 0 ], # go down\n",
    "         [ 0, 1 ]] # go right\n",
    "\n",
    "# for printing the path\n",
    "def show(grid):\n",
    "    for l in grid:\n",
    "        print(l)\n",
    "\n",
    "# A* algorithm\n",
    "def search(grid, init, goal, cost, heuristic):\n",
    "    closed = [[0 for col in range(len(grid[0]))] for row in range(len(grid))]\n",
    "    closed[init[0]][init[1]] = 1\n",
    "\n",
    "    expand = [[-1 for col in range(len(grid[0]))] for row in range(len(grid))]\n",
    "    action = [[-1 for col in range(len(grid[0]))] for row in range(len(grid))]\n",
    "\n",
    "    x = init[0]\n",
    "    y = init[1]\n",
    "    g = 0\n",
    "    h = heuristic[x][y]\n",
    "    f = g+h\n",
    "    open = [[f, g, x, y]]\n",
    "\n",
    "    found = False  # flag that is set when search is complete\n",
    "    resign = False # flag set if we can't find expand\n",
    "    count = 0\n",
    "    \n",
    "    while not found and not resign:\n",
    "        if len(open) == 0:\n",
    "            resign = True\n",
    "            return \"Fail\"\n",
    "        else:\n",
    "            open.sort()\n",
    "            open.reverse()\n",
    "            next = open.pop()\n",
    "            x = next[2]\n",
    "            y = next[3]\n",
    "            g = next[1]\n",
    "            expand[x][y] = count\n",
    "            count += 1\n",
    "            \n",
    "            if x == goal[0] and y == goal[1]:\n",
    "                found = True\n",
    "            else:\n",
    "                for i in range(len(delta)):\n",
    "                    x2 = x + delta[i][0]\n",
    "                    y2 = y + delta[i][1]\n",
    "                    if x2 >= 0 and x2 < len(grid) and y2 >=0 and y2 < len(grid[0]):\n",
    "                        if closed[x2][y2] == 0 and grid[x2][y2] == 0:\n",
    "                            g2 = g + cost\n",
    "                            f2 = g + cost + heuristic[x2][y2]\n",
    "                            open.append([f2, g2, x2, y2])\n",
    "                            closed[x2][y2] = 1\n",
    "\n",
    "    return expand\n",
    "    \n",
    "l = search(grid,init,goal,cost,heuristic)\n",
    "show(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above output -1 represent, that node is never expanded. So we can see that A* search expands only minimal number of nodes to reach to that goal location. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative method for planning is called dynamic programming. It has both advantages and disadvantages. It also gives us the shortest path like the A* search. But it outputs best path from any position of the grid, not just the best path from initial location. This is important because real world is very stochastic which means based on the current situation of the environment sometimes the robot needs to change its path. Then the initial location might be changed. So we need a way to select the best path from any location of the environment. \n",
    "\n",
    "What dynamic programming gives us is a policy to map every grid location to an action. So whatever the location is, we will know what action to take to go to the goal state from that location. To come up with this kind of policy first we have to compute something called value function. Which will assign value to each valid grid (location that contains no obstacles) such as it represents the lowest  cost to reach the goal postion from that location. For example, consider the following world:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gird = [['S' , 0 , 0, 0], # S is the initial location\n",
    "        [0 , 1 , 0, 0 ],\n",
    "        [0 , 1 , 0, 0],\n",
    "        [0 , 0 , 0, 'G']] # G is the goal location\n",
    "# This should be the output of the value function\n",
    "value = [[6 , 5 , 4, 3], # 99 means obstacles\n",
    "         [5 , 99 , 3, 2],\n",
    "         [4 , 99 , 2, 1],\n",
    "         [3 , 2 , 1, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we get the values we can easily convert it to policy like the follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v', ' ', 'v', 'v', 'v', 'v']\n",
      "['v', ' ', 'v', 'v', 'v', 'v']\n",
      "['v', ' ', 'v', 'v', 'v', 'v']\n",
      "['v', ' ', '>', '>', '>', 'v']\n",
      "['>', '>', '^', '^', ' ', '*']\n"
     ]
    }
   ],
   "source": [
    "grid = [[0, 1, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 1, 0, 0, 0, 0],\n",
    "        [0, 0, 0, 0, 1, 0]]\n",
    "init = [0, 0] # initial location\n",
    "goal = [len(grid)-1, len(grid[0])-1] # goal location\n",
    "cost = 1 # the cost associated with moving from a cell to an adjacent one\n",
    "\n",
    "delta = [[-1, 0 ], # go up\n",
    "         [ 0, -1], # go left\n",
    "         [ 1, 0 ], # go down\n",
    "         [ 0, 1 ]] # go right\n",
    "\n",
    "delta_name = ['^', '<', 'v', '>']\n",
    "\n",
    "def show(grid):\n",
    "    for r in grid:\n",
    "        print(r)\n",
    "\n",
    "def optimum_policy(grid,goal,cost):\n",
    "    value = [[99 for row in range(len(grid[0]))] for col in range(len(grid))]\n",
    "    policy = [[' ' for row in range(len(grid[0]))] for col in range(len(grid))]\n",
    "    change = True\n",
    "    while change:\n",
    "        change = False\n",
    "        for x in range(len(grid)):\n",
    "            for y in range(len(grid[0])):\n",
    "                if goal[0] == x and goal[1] == y:\n",
    "                    if value[x][y] > 0:\n",
    "                        value[x][y] = 0\n",
    "                        # save the goal state\n",
    "                        policy[x][y] = '*'\n",
    "                        change = True\n",
    "                elif grid[x][y] == 0:\n",
    "                    for a in range(len(delta)):\n",
    "                        x2 = x + delta[a][0]\n",
    "                        y2 = y + delta[a][1]\n",
    "\n",
    "                        if x2 >= 0 and x2 < len(grid) and y2 >= 0 and y2 < len(grid[0]) and grid[x2][y2] == 0:\n",
    "                            v2 = value[x2][y2] + cost\n",
    "\n",
    "                            if v2 < value[x][y]:\n",
    "                                change = True\n",
    "                                value[x][y] = v2\n",
    "                                # save the action for which we are saving the value\n",
    "                                policy[x][y] = delta_name[a]\n",
    "\n",
    "    return policy\n",
    "res = optimum_policy(grid, goal, cost)\n",
    "show(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above output we can see that the dynamic programming gives us policy for each grid position. Till now we consider discrete motion. Next we will learn continuous motion planning and control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robot Motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above discussion we saw how to get an optimal path for robot. In this section we will learn how to convert this path to actual robot motion. In particular we will talk about generating smooth path and then control (PID control). \n",
    "\n",
    "<b>Generating smooth path</b>\n",
    "In the previous section we consider the world to be discrete and describe the optimal path as a sequence of points. Describing path like this is not suitable for robot motion. Some robots like autonomous car can not even follow this type of path(because of sharp turn). So we need to smooth this path in such a way that a robot motion can follow the path. \n",
    "\n",
    "The smoothing algorithm is as follows: \n",
    "\n",
    "step 1: \n",
    "\n",
    "    Xi = Yi (X is a discrete point from planner and Y is the smoothed point)\n",
    "    \n",
    "step 2:\n",
    "    Minimize:\n",
    "    $$(X - Y)^{2}$$\n",
    "    $$(Y_{i} - Y_{i+1})^{2}$$ \n",
    "    (Yi and Yi+1 are the two consecutive points in the smooth path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we closely look into the algortihm, we can see that the first equation tries to minimize the difference between smoothed point and discrete point whereas the second equation tries to minimize difference between two consecutive smoothed points. Obviously this is opposite to each other. So we have to include a balance factor between this two equation. We will combine these two equation first and then try to optimize the combined equation with gradient descent algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combined form of this two equation is as follows:\n",
    "\n",
    "$$Y_{i} = Y_{i} + \\alpha(X_{i}-Y_{i}) + \\beta(Y_{i-1}+Y_{i+1}-2Y_{i})$$\n",
    "\n",
    "We have to apply the algorithm fixing the starting and goal location, because we do not want to change those. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "path = [[0, 0], # sample discrete path\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [1, 2],\n",
    "        [2, 2],\n",
    "        [3, 2],\n",
    "        [4, 2],\n",
    "        [4, 3],\n",
    "        [4, 4]]\n",
    "\n",
    "def smooth(path, weight_data = 0.5, weight_smooth = 0.1, tolerance = 0.000001):\n",
    "    # Make a deep copy of path into newpath\n",
    "    newpath = deepcopy(path)\n",
    "    update = tolerance # based on this we will stop the gradient descent\n",
    "    while update>=tolerance:\n",
    "        update = 0.0\n",
    "        for i in range(1,len(newpath)-1):\n",
    "            for j in range(len(newpath[0])):\n",
    "                temp = newpath[i][j]\n",
    "                newpath[i][j] += weight_data * (path[i][j] - newpath[i][j]) +\\\n",
    "                    weight_smooth * (newpath[i-1][j] + newpath[i+1][j] - 2.0 * newpath[i][j])\n",
    "                update += abs(temp-newpath[i][j])\n",
    "    \n",
    "    return newpath\n",
    "\n",
    "path_arr = np.array(path)\n",
    "smooth_path = np.array(smooth(path))\n",
    "\n",
    "plt.plot(path_arr[:,0],path_arr[:,1],label='discrete path')\n",
    "plt.plot(smooth_path[:,0],smooth_path[:,1],label='smoothed path')\n",
    "plt.title('smooth plot')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can see that we convert the discrete values to continuous values except for starting and ending point. Thus we find a path that is suitable for robot motion. However there are some caveats here, whenever there is obstacles in the environment and if we do not consider the obstacles during planning or smoothing, The smooth path can hit the obstacles. So during smoothing or planning the path we have to consider about the obstacles based on our environment of the robot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>PID control: </b> So we got an optimal path from planner and then applying smoothing to the path we got a smooth path suitable for our robot motion. Now we have to give the robot command to follow the smoothed path. How can we do that? For example, an autonomous car has two steerable wheels(at the front) and two stationary wheels(at the back). We can control the steer such as the car follow the smoothed path. For that purpose we can calculate the cross track error(CTE) (distance between the middle point of the car and the reference trajectory(smoothed path)) and steer in proportion to CTE. This controller is known as P controller. So the steering angle will be -\n",
    "    \n",
    "   $$\\alpha = -\\tau * CTE$$ here $\\tau$ is proportional gain. \n",
    "    \n",
    "    \n",
    "There are some issues involved in doing this. For example, if we change the steer angle in proportion to the CTE then eventually the car will overshoot the reference trajectory and the motion for the car will be something like a sin curve with a very little amplitude. Now consider yourself sitting in that car :p. That will be very unrealistic right? \n",
    "    \n",
    "So now we have to find a way to remove this oscillation. The trick is called PD control. In PD control, the steering angle is not just proportional to the CTE but also to the differential CTE. This means when the car has turned enough to reduce the CTE it won't just go shooting for the X axis but it will notice that it already reducing the error by this differential term. So when the error becomes smaller over time it counter steers and steers up again. This will allow the car to gracefully reach the target trajectory. PD controller uses another control parameter called differential gain. For PD controller the steering action will be- \n",
    "\n",
    "   $$\\alpha = -\\tau_{p} * CTE -\\tau_{d} * \\frac{d}{dt}CTE$$ here $\\tau_{d}$ is proportional gain. \n",
    "\n",
    "The differenttial term can be calculated very easily by keeping record of CTE. At time t the differential term will be-\n",
    "\n",
    "   $$\\frac{d}{dt} = \\frac{CTE_{t} - CTE_{t-1}}{\\Delta t}$$\n",
    "  \n",
    "A very common problem that often occurs in robotic is called systemetic bias. Suppose the wheel of the car has some steering drift noise. By that we mean that the control of the wheel has some errors. Then the car will not converge to the trajectory line anymore. It will have a bias problem. It will keep a constant distance from trajectory line because of steering drift noise. To solve this we will use PID controller. \n",
    "\n",
    "In PID controller we will notice the CTE over a long period of time and if the CTE does not significantly become low then we will steer more and more with time goes by, so that the car converges to the trajectory line after certain time even with steering drift noise. So the equation is as follows-\n",
    "\n",
    "   $$\\alpha = -\\tau_{p} * CTE -\\tau_{d} * \\frac{d}{dt}CTE -\\tau_{i} * \\sum CTE $$here $\\tau_{i}$ is integration gain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pid.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following is the code for PID controller implementation. We use similar robot class from Particle filter example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fe57a9c9c50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Robot(object):\n",
    "    def __init__(self, length=20.0):\n",
    "        \"\"\"\n",
    "        Creates robot and initializes location/orientation to 0, 0, 0.\n",
    "        \"\"\"\n",
    "        self.x = 0.0\n",
    "        self.y = 0.0\n",
    "        self.orientation = 0.0\n",
    "        self.length = length\n",
    "        self.steering_noise = 0.0\n",
    "        self.distance_noise = 0.0\n",
    "        self.steering_drift = 0.0\n",
    "\n",
    "    def set(self, x, y, orientation):\n",
    "        \"\"\"\n",
    "        Sets a robot coordinate.\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.orientation = orientation % (2.0 * np.pi)\n",
    "\n",
    "    def set_noise(self, steering_noise, distance_noise):\n",
    "        \"\"\"\n",
    "        Sets the noise parameters.\n",
    "        \"\"\"\n",
    "        # makes it possible to change the noise parameters\n",
    "        # this is often useful in particle filters\n",
    "        self.steering_noise = steering_noise\n",
    "        self.distance_noise = distance_noise\n",
    "\n",
    "    def set_steering_drift(self, drift):\n",
    "        \"\"\"\n",
    "        Sets the systematical steering drift parameter\n",
    "        \"\"\"\n",
    "        self.steering_drift = drift\n",
    "\n",
    "    def move(self, steering, distance, tolerance=0.001, max_steering_angle=np.pi / 4.0):\n",
    "        \"\"\"\n",
    "        steering = front wheel steering angle, limited by max_steering_angle\n",
    "        distance = total distance driven, most be non-negative\n",
    "        \"\"\"\n",
    "        if steering > max_steering_angle:\n",
    "            steering = max_steering_angle\n",
    "        if steering < -max_steering_angle:\n",
    "            steering = -max_steering_angle\n",
    "        if distance < 0.0:\n",
    "            distance = 0.0\n",
    "\n",
    "        # apply noise\n",
    "        steering2 = random.gauss(steering, self.steering_noise)\n",
    "        distance2 = random.gauss(distance, self.distance_noise)\n",
    "\n",
    "        # apply steering drift\n",
    "        steering2 += self.steering_drift\n",
    "\n",
    "        # Execute motion\n",
    "        turn = np.tan(steering2) * distance2 / self.length\n",
    "\n",
    "        if abs(turn) < tolerance:\n",
    "            # approximate by straight line motion\n",
    "            self.x += distance2 * np.cos(self.orientation)\n",
    "            self.y += distance2 * np.sin(self.orientation)\n",
    "            self.orientation = (self.orientation + turn) % (2.0 * np.pi)\n",
    "        else:\n",
    "            # approximate bicycle model for motion\n",
    "            radius = distance2 / turn\n",
    "            cx = self.x - (np.sin(self.orientation) * radius)\n",
    "            cy = self.y + (np.cos(self.orientation) * radius)\n",
    "            self.orientation = (self.orientation + turn) % (2.0 * np.pi)\n",
    "            self.x = cx + (np.sin(self.orientation) * radius)\n",
    "            self.y = cy - (np.cos(self.orientation) * radius)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '[x=%.5f y=%.5f orient=%.5f]' % (self.x, self.y, self.orientation)\n",
    "# initialize robot\n",
    "robot = Robot()\n",
    "# set initialize position\n",
    "robot.set(0, 1, 0)\n",
    "\n",
    "# applyiong PID control on the move function of the robot to control the robot motion\n",
    "def run(robot, tau_p, tau_d, tau_i, n=100, speed=1.0):\n",
    "    x_trajectory = []\n",
    "    y_trajectory = []\n",
    "    # TODO: your code here\n",
    "    prev_cte = robot.y\n",
    "    int_cte = 0\n",
    "    for i in range(n):\n",
    "        cte = robot.y\n",
    "        diff_cte = (cte - prev_cte) / speed\n",
    "        prev_cte = cte\n",
    "        int_cte += cte\n",
    "        steer = -tau_p * cte - tau_d * diff_cte - tau_i * int_cte\n",
    "        robot.move(steer, speed)\n",
    "        x_trajectory.append(robot.x)\n",
    "        y_trajectory.append(robot.y)\n",
    "    return x_trajectory, y_trajectory\n",
    "\n",
    "\n",
    "x_trajectory, y_trajectory = run(robot, 0.2, 3.0, 0.004)\n",
    "n = len(x_trajectory)\n",
    "# plotting robot trajectory\n",
    "plt.plot(x_trajectory, y_trajectory, 'g', label='PID controller')\n",
    "plt.plot(x_trajectory, np.zeros(n), 'r', label='reference')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the equation above we can see that the control gains ($\\tau_{d}\\tau_{i}\\tau_{p}$) are very important constant to choose and the performance depends on these control gains. So how can we come up with a good values for these? There are many hyperparameter optimization techniques. Twiddle (also known as Coordinate Ascent) is one of them. But it is very easy to understand.\n",
    "\n",
    "Twiddle Algorithm:\n",
    "<img src=\"images/twiddle.png\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here the goodness is measured by the CTE by running the move function for a candidate set of parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SLAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SLAM is a very popular technique for doing localization and mapping at the same time as the name suggest SLAM(Simultenious Localization And Mapping). So in the previous section when we learn about localization we consider that we already have the map of the environment. Now we will localize and create the map at the same time using SLAM techniques. Localization is important in creating map. Otherwise if the robot runs into a loop because of some drift error or else the resulted map will be very different then reality. \n",
    "\n",
    "There are many version of SLAM algorithm. We will discuss about graph SLAM this is very easy to understand and also very effective. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand this in a simple way let’s assume an initial position of the robot X=0 & Y=0, for this example assume that there is no concern of heading direction & compass. Let’s assume the robot moves to the right in the x-direction by 10, so now the location after motion(X1) in a perfect world would be X1= X0+10 and Y1= 0.\n",
    "\n",
    "But as we know that the real world is uncertain. So it is better to represent the robot location with gaussian distribution centered around 10 with some uncertainty. So if we put this equation into Gaussian formula we will get the probability distribution of the new location which is as follows- \n",
    "\n",
    "$$ p(X_{1}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}*\\exp{-\\frac{1}{2}*\\frac{(X_{1}-X_{0}-10)^{2}}{\\sigma^{2}}}$$\n",
    "$$ p(Y_{1}) = \\frac{1}{\\sqrt{2\\pi\\sigma^{2}}}*\\exp{-\\frac{1}{2}*\\frac{(Y_{1}-Y_{0})^{2}}{\\sigma^{2}}}$$\n",
    "\n",
    "The goal is to maximize the likelihood of the position X1 given the position X0 is (0,0).The product of these two Gaussian is now our constraint. So, what GRAPH SLAM does is it defines the probabilities using a sequence of such constraints. \n",
    "\n",
    "GRAPH SLAM collects it’s initial location which is (0,0) initially (Initial Constraints) then lots of relative constraints that relate each robot pose to the previous robot pose( Relative Motion Constraints). As an example, let’s use landmarks which can be seen by the robot at various locations which would be Relative Measurement Constraints(every time a robot sees a landmark).\n",
    "\n",
    "These constraints altogether find the most likely configuration of the robot path along with the location of landmark and that is the mapping process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this algorithm we maintain one matrix called $\\Omega$ and another vector of distances. Then we move the robot and add the corresponding constraint in the matrix (This is based on total probability theory). We also multiply the constrain with the uncertainty term $\\frac{1}{\\sigma^{2}}$ before adding it to the matrix. It simply increase the confidence of the constrain. At the end we take the inverse of the matrix and then multiply with the vecto and thus we get a vector of all possible location of the robot and Landmarks. The algorithm is demostrated in the following figure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "       <tr>\n",
    "            <th><img src=\"images/slam.png\" width=\"400\"/></th>\n",
    "            <th><img src=\"images/slam1.png\" width=\"400\"/></th>\n",
    "       </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above simple technique has a very great disadvantage though. If you notice the matrix, it has one column and row for every single step the robot takes. So with time this matrix grow very large and eventually it will stop working. The solution for this is online SLAM. What we do in online SLAM is we keep only the last position information and landmark information (landmarks are fixed) in the matrix. At first we expand the matrix with one row and column to calculate the values in the same way. After calculating we discard the previous step information and keep only the latest step information. Before discarding we do some calculation to include the previous step information inside the remaining matrix. Online SLAM is explained in the following figure: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/online_slam.png\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus the course ends here. Some sample projects that applies these theories in practical can be found in my github profile. (Link will be given here soon)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
