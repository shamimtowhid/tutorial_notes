{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains all my notes from Udacity Course [Artificial Intelligence for Robotics](https://www.udacity.com/course/artificial-intelligence-for-robotics--cs373).  I have learned a lots of fundamental concept of autonomous driving from this course and I am thankful to Udacity for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Localization is the process, how a robot identify its location in an environment. A robot does this using probability theory. Initially the robot does not know where it is, so the probability of being in any place will be the same. In localization, basically we try to change these probabilities with the measurement of the robot such as the probability of one place increases whereas probability of other places decreases. The location with highest probability is the location of the robot and thus our robot will localize itself in the environment. \n",
    "\n",
    "##### steps in localization are:\n",
    "\n",
    "   1. Initialize all the locations with uniform probability distribution, which is called prior belief.\n",
    "   2. sense the world and change our prior belief according to the sense, which is called posterior belief. To change our prior we have to multiply it with the factor of the sense being correct or wrong. Further we have to normalize this to make it a valid probability distribution.   \n",
    "   3. Take an action and change the posterior belief according to the action taken, which is called convolution. \n",
    "   4. sense the world again and now changing our prior belief from previous step will do the trick, now our posterior belief will be something meaningfull. Only one location will have the highest probability. So thus our robot will localize itself. \n",
    "\n",
    "To understand it, lets assume a robot in one dimensional discrete world. We can define the location of this world with different colors and our robot can sense these colors. Now we will try to apply the above algorithm on this robot and lets whether our robot can localize itself or not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13636363636363638, 0.13636363636363638, 0.4090909090909091, 0.13636363636363638, 0.04545454545454547, 0.13636363636363638]\n"
     ]
    }
   ],
   "source": [
    "# one dimensional world\n",
    "world = ['red', 'green', 'green','red', 'red', 'green']\n",
    "# step 1: Initializing all the locations with uniform probability\n",
    "p = [1/len(world) for _ in range(len(world))]\n",
    "# These are the factor of being the sense correct or not \n",
    "pHit = 0.6 \n",
    "pMiss = 0.2\n",
    "\n",
    "# These are the valid action the robot can take\n",
    "action = [0,1] #0 means move left and 1 means move right\n",
    "\n",
    "# we will do step 2 as a function because we have to do it repeatedly.\n",
    "def sense(sense, prior):\n",
    "    '''This function can calculate posterior probability given the sense and prior'''\n",
    "    posterior = [0.0 for _ in range(len(prior))]\n",
    "#     chaning prior according to the factor\n",
    "    for i in range(len(prior)):\n",
    "        if sense==world[i]:\n",
    "            posterior[i] = prior[i] * pHit\n",
    "        else:\n",
    "            posterior[i] = prior[i] * pMiss\n",
    "#     normalizing the posterior to make it a valid probability distribution\n",
    "    posterior = [p/sum(posterior) for p in posterior]\n",
    "    return posterior\n",
    "\n",
    "\n",
    "# we will do step 3 as a function too, cause we have to do it repeatedly too. \n",
    "def convolution(action, posterior):\n",
    "    '''This function will change the posterior according to the action taken by the robot.\n",
    "    This function assumes the world to be a cyclic world that means the element falls off \n",
    "    from right will go to the left or vice versa'''\n",
    "    new_prior = [0.0 for _ in range(len(posterior))]\n",
    "#   robot moves to the left\n",
    "    if action == 0:\n",
    "        new_prior = [posterior[(i+1)%len(posterior)] for i in range(len(posterior))]\n",
    "#     robot moves to the right\n",
    "    else:\n",
    "        new_prior = [posterior[(i-1)] for i in range(len(posterior))]\n",
    "    return new_prior\n",
    "\n",
    "# applying the algorithm\n",
    "# step 2\n",
    "posterior = sense('green',p)\n",
    "# step 3\n",
    "new_prior = convolution(1, posterior)\n",
    "# step 2 repeated\n",
    "new_posterior = sense('green',new_prior)\n",
    "print(new_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see the highest probability is in location 2(starting from 0 location) which is correct according to our world setup, if we sense two times and green is the result for each time. So by applying the above algorithm our robot can actually localize itself in this one dimensional world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inexact robot motion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have undersatnd the basic concepts of localization. But in our previous example we assume the robot to be in exact motion. Which means if the robot wants to go to right by one cell it does it correctly every time. This is not the case in real life. In reality, the robot motion is uncertain. \n",
    "\n",
    "So for example, lets say if we give the robot the command to move right by one cell, the robot remains in the current cell with probability of 0.1, it moves to the right cell with probability of 0.8, it can also overshoot the goal by one cell with probability of 0.1. \n",
    "\n",
    "Considering this uncertain robot motion, the new probability of a cell will be the addition of all the probabilities from cells those are candidate for the current cell.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21157894736842103, 0.1515789473684211, 0.08105263157894739, 0.16842105263157897, 0.3873684210526316]\n"
     ]
    }
   ],
   "source": [
    "world=['green', 'red', 'red', 'green', 'green']\n",
    "p = [1/len(world) for _ in range(len(world))]\n",
    "measurements = ['red', 'green'] # considering multiple measrements\n",
    "motions = [1, 1] # considering multiple motion\n",
    "pHit = 0.6\n",
    "pMiss = 0.2\n",
    "# probability for inexact robot motion\n",
    "pExact = 0.8 # probability for the correct move\n",
    "pOvershoot = 0.1 # probability for overshooting the goal \n",
    "pUndershoot = 0.1 # probability for undershooting the goal\n",
    "\n",
    "def sense(sense, prior):\n",
    "    '''This function can calculate posterior probability given the sense and prior'''\n",
    "    posterior = [0.0 for _ in range(len(prior))]\n",
    "#     chaning prior according to the factor\n",
    "    for i in range(len(prior)):\n",
    "        if sense==world[i]:\n",
    "            posterior[i] = prior[i] * pHit\n",
    "        else:\n",
    "            posterior[i] = prior[i] * pMiss\n",
    "#     normalizing the posterior to make it a valid probability distribution\n",
    "    posterior = [p/sum(posterior) for p in posterior]\n",
    "    return posterior\n",
    "\n",
    "def convolution(p, U):\n",
    "    '''This function takes the posterior probability and steps to move in left or right.\n",
    "    It returns new prior distribution. U=1 means move right by one cell, U=-1 means move left \n",
    "    by one cell. Assuming the world to be cyclic'''\n",
    "    q = []\n",
    "    for i in range(len(p)):\n",
    "        s = pExact * p[(i-U) % len(p)] # calculating probability of correct motion\n",
    "        s = s + pOvershoot * p[(i-U-1) % len(p)] # probability of overshoot motion\n",
    "        s = s + pUndershoot * p[(i-U+1) % len(p)] # probability of undershoot motion\n",
    "        q.append(s)\n",
    "    return q\n",
    "\n",
    "for i,s in enumerate(measurements):\n",
    "    # step 2\n",
    "    p = sense(s,p)\n",
    "    # step 3\n",
    "    p = convolution(p, motions[i])\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above output we can see that the probability distribution works in same way as previous but this time we consider the inexact robot motion. So this convolution function or move function is more accurate considering the real life scenario. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### understand sense and move from probability theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely to the sense function it will lead us to the Bayesian rule. Lets represent the prior belief as X and the measurement as Z then the sense function is calculating the probability after having the measurement Z. In mathmatically we can say it like p(X|Z). We all know that according to the bayes theorem, \n",
    "\n",
    "    p(X|Z) = (p(Z|X) * P(X)) / p(Z)\n",
    "    \n",
    "Here p(Z|X) is the probability of having a measurement. In our case it is defined by pHit and pMiss. p(X) is the prior belief. p(Z) is just a normalized term. Since our final output is a posterior distribution we can replace p(Z) with just the normalization term. Thats the beauty of Bayes rule. So, \n",
    "\n",
    "    p(Z) = sum of p(Z|X) * P(X) for all the cells. \n",
    "    \n",
    "Now the move or convolution function can be relate to something called total probability theory. The way we computed one cell probability after one move, was looking at all the grid cells from which it could have come from one time step earlier, we looked at the prior probability of those grid cells at previous time step and we multiply it with a probability that our motion command would carry us from those cells to this current cell. In probability term people write this like the follows-\n",
    "\n",
    "    p(A) = sum over all B cells ( p(A|B) p(B) ) , here p(B) is the prior at previous time step , p(A|B) is probability of transition to this cell from B cells. \n",
    "    \n",
    "Here A is the current cell index and B is the all possible previous cell's prior probabilities. This theorem is known as Theorem of total probability.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
